"""
engines/vulnerability/orchestrator.py

M-ILEA Master Orchestrator (v3)

Full static + optional dynamic analysis pipeline:

  [0]  Adapter stage  (apktool, jadx, native extraction)
  [1]  Evidence pipeline  (smali/manifest/native evidence extractor)
  [2]  Vulnerability signal scanners  (M1-M10 OWASP MSTG)
  [3]  Vulnerability engine  (evidence-based + signal-based findings)
  [4]  ARA SCAN  (SmaliARAScanner + NativeLibraryAnalyzer + ADBDynamicAdapter)
  [5]  ARA profile build  (UnifiedProtectionProfiler)
  [6]  Correlation  (ARA vs Vulnerabilities)
  [7]  Risk scoring
  [8]  Unified report (JSON + HTML)
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

# Adapter Layer
from adapters.context import AdapterContext
from adapters.apktool.adapter import ApktoolAdapter
from adapters.jadx.adapter import JadxAdapter
from adapters.native.adapter import NativeLibAdapter
from adapters.native.analyzer import NativeLibraryAnalyzer
from adapters.adb.adapter import ADBDynamicAdapter

# Evidence Layer
from core.evidence.context import EvidenceContext
from core.evidence.smali import SmaliEvidenceExtractor
from core.evidence.native import NativeEvidenceExtractor
from core.evidence.manifest import ManifestEvidenceExtractor

# ARA Scanner
from core.strategy.smali_ara_scanner import (
    SmaliARAScanner,
    ARAFinding,
    build_ara_summary_from_findings,
)
from core.strategy.unified_profile import UnifiedProtectionProfiler

# New: Atom/Fingerprint & Call Graph
from core.atoms.registry import FingerprintRegistry
from core.callgraph.analyzer import CallGraphAnalyzer

# New: Dynamic Analyzer (ADB-based, non-invasive)
from core.dynamic.analyzer import DynamicAnalyzer

# New: Summary Analyzer (cross-layer fusion)
from core.summary.analyzer import SummaryAnalyzer

# Vulnerability Signal Scanners
from engines.vulnerability.models import VulnerabilitySignal

# Manifest-based scanners (XML — not part of unified smali scan)
from engines.vulnerability.manifest.scanner import AndroidManifestScanner
from engines.vulnerability.manifest.exported_scanner import ExportedComponentScanner
from engines.vulnerability.manifest.intent_hijack_scanner import IntentHijackingScanner
from engines.vulnerability.manifest.permission_scanner import DangerousPermissionScanner
from engines.vulnerability.manifest.cleartext_scanner import CleartextTrafficScanner
from engines.vulnerability.network.netsec_scanner import NetworkSecurityConfigScanner

# Unified single-pass scanner (replaces multi-pass vuln + ARA smali scans)
from engines.vulnerability.unified_scanner import UnifiedSmaliScanner

# JADX Java source scanner (human-readable vulnerability evidence)
from engines.vulnerability.jadx_scanner import JadxVulnerabilityScanner

# Vulnerability Engine + Report
from engines.vulnerability.engine import VulnerabilityEngine
from engines.vulnerability.findings import VulnerabilityFinding
from engines.vulnerability.correlation.engine import CorrelationEngine
from engines.vulnerability.risk.engine import RiskScoringEngine
from engines.report.builder import UnifiedReportBuilder
from engines.report.html_generator import UnifiedHTMLReportGenerator


# Bump this whenever the analysis pipeline adds new modules / sections.
# Old cached results with a different version will be automatically invalidated.
PIPELINE_VERSION = "4.2-callgraph-category-fix"


class M_ILEAOrchestrator:
    """
    Master Orchestrator for M-ILEA v3.
    Full static (Smali + .so) and optional dynamic (ADB) pipeline.
    """

    def __init__(self):
        self.apktool = ApktoolAdapter()
        self.jadx    = JadxAdapter()
        self.native  = NativeLibAdapter()

        self.smali_extractor    = SmaliEvidenceExtractor()
        self.native_extractor   = NativeEvidenceExtractor()
        self.manifest_extractor = ManifestEvidenceExtractor()

        # Manifest / XML-based scanners (not in unified smali scan)
        self.manifest_scanner   = AndroidManifestScanner()
        self.exported_scanner   = ExportedComponentScanner()
        self.intent_scanner     = IntentHijackingScanner()
        self.permission_scanner = DangerousPermissionScanner()
        self.cleartext_scanner  = CleartextTrafficScanner()
        self.netsec_scanner     = NetworkSecurityConfigScanner()

        self.smali_ara_scanner   = SmaliARAScanner()
        self.native_ara_analyzer = NativeLibraryAnalyzer()
        self.adb_adapter         = ADBDynamicAdapter()

        # New: Atom/Fingerprint + Call Graph + Dynamic + Summary
        self.fingerprint_registry = FingerprintRegistry()
        self.callgraph_analyzer   = CallGraphAnalyzer(registry=self.fingerprint_registry)
        self.dynamic_analyzer     = DynamicAnalyzer()
        self.summary_analyzer     = SummaryAnalyzer()

        self.unified_scanner     = UnifiedSmaliScanner()
        self.jadx_scanner        = JadxVulnerabilityScanner()

        self.profiler            = UnifiedProtectionProfiler()

        self.vuln_engine        = VulnerabilityEngine()
        self.correlation_engine = CorrelationEngine()
        self.risk_engine        = RiskScoringEngine()
        self.report_builder     = UnifiedReportBuilder()
        self.html_generator     = UnifiedHTMLReportGenerator()

    # ==================================================================
    # PUBLIC ENTRY POINT
    # ==================================================================

    def analyze(
        self,
        workspace_path: str,
        metadata: Optional[Dict[str, Any]] = None,
        progress_cb=None,               # Optional[Callable[[float, str], None]]
    ) -> Dict[str, Any]:
        import hashlib
        import concurrent.futures

        def _progress(pct: float, msg: str) -> None:
            logger.info(f"  [{pct:.0f}%] {msg}")
            if progress_cb:
                try:
                    progress_cb(pct, msg)
                except Exception:
                    pass

        workspace = Path(workspace_path).resolve()
        if metadata is None:
            metadata = {"workspace": str(workspace), "version": "3.0"}

        # ── [0a] APK selection + quick file info ───────────────────────────
        logger.info("[0/8] Adapter Stage")
        _progress(2, "Resolving APK")
        base_apk   = self._resolve_base_apk(workspace)
        size_bytes = base_apk.stat().st_size
        metadata.setdefault("file_name",       base_apk.name)
        metadata.setdefault("file_size_bytes", size_bytes)
        metadata.setdefault("file_size_mb",    f"{size_bytes / (1024 * 1024):.2f} MB")

        # ── [0b] SHA-256 fingerprint (single pass — cache key) ─────────────
        _progress(4, f"Fingerprinting APK ({metadata['file_size_mb']})")
        _sha256_h = hashlib.sha256()
        with open(base_apk, "rb") as _f:
            for _chunk in iter(lambda: _f.read(131072), b""):
                _sha256_h.update(_chunk)
        apk_sha256 = _sha256_h.hexdigest().upper()
        metadata.setdefault("file_sha256", apk_sha256)
        logger.info(f"  [sha256] {apk_sha256[:16]}…")

        # ── [0c] APK-level result cache ────────────────────────────────────
        _progress(6, "Checking analysis cache")
        _cached = self._check_apk_cache(workspace.parent, apk_sha256)
        if _cached and _cached.get("_pipeline_version") == PIPELINE_VERSION:
            # Stamp session-specific fields then regenerate HTML (instant)
            _cached.get("metadata", {}).update(
                {k: v for k, v in metadata.items() if k in ("session_id", "workspace")}
            )
            _progress(92, "Cache hit — rendering report")
            html = self.html_generator.generate(_cached)
            _progress(100, "Analysis complete (from cache)")
            return {"report": _cached, "html": html}
        elif _cached:
            logger.info("  [cache] Stale pipeline version — re-analysing")

        # ── [0d] MD5 + SHA1 in background (finishes during decompilation) ──
        _hash_exec   = concurrent.futures.ThreadPoolExecutor(
            max_workers=1, thread_name_prefix="apk-hash"
        )
        def _slow_hashes():
            _m = hashlib.md5(); _s = hashlib.sha1()
            with open(base_apk, "rb") as _f:
                for _chunk in iter(lambda: _f.read(131072), b""):
                    _m.update(_chunk); _s.update(_chunk)
            return _m.hexdigest().upper(), _s.hexdigest().upper()
        _hash_future = _hash_exec.submit(_slow_hashes)

        # ── [0e] Decompile (skip if artifacts already exist) ───────────────
        adapter_ctx = AdapterContext(
            session_id=workspace.name,
            apk_path=base_apk,
            workspace_root=workspace,
        )
        if self._decompile_artifacts_exist(adapter_ctx):
            _progress(40, "Reusing cached decompilation artifacts")
            logger.info("  [apktool] artifacts exist — skipping decompilation")
        else:
            _progress(8, f"Decompiling APK — {base_apk.name}")
            self._run_adapters(adapter_ctx)
            _progress(40, "Decompilation complete")

        # ── Enrich from manifest + cert (skips hashing — already done above) ─
        self._enrich_metadata(adapter_ctx.decompile_dir, base_apk, metadata)

        # ── Collect MD5 + SHA1 (should be ready) ──────────────────────────
        try:
            _md5, _sha1 = _hash_future.result(timeout=120)
            metadata.setdefault("file_md5",  _md5)
            metadata.setdefault("file_sha1", _sha1)
            logger.info(f"  [hash] md5={_md5[:8]}… sha1={_sha1[:8]}…")
        except Exception as _he:
            logger.warning(f"  [hash] md5/sha1 failed: {_he}")
        finally:
            _hash_exec.shutdown(wait=False)

        # ── [1] Evidence ───────────────────────────────────────────────────
        _progress(43, "Extracting evidence")
        logger.info("[1/8] Evidence Pipeline")
        ev_ctx = EvidenceContext(
            decompiled_dir=adapter_ctx.decompile_dir,
            jadx_dir=adapter_ctx.jadx_dir,
            native_dir=adapter_ctx.native_dir,
        )
        self.smali_extractor.extract(ev_ctx)
        self.native_extractor.extract(ev_ctx)
        self.manifest_extractor.extract(ev_ctx)
        evidence = ev_ctx.evidence
        logger.info(f"  {len(evidence)} evidence items")

        # ── [2+4a] Unified smali scan (single-pass vuln + ARA, ProcessPool) ─
        _progress(48, "Running unified smali scan (single-pass, multi-process)")
        logger.info("[2+4a/8] Unified Smali Scan (vuln + ARA)")
        decompile_dir = adapter_ctx.decompile_dir

        def _unified_progress(pct, msg):
            # Map unified scanner 0-100 → orchestrator 48-72
            _progress(48 + pct * 0.24, msg)

        all_signals, smali_ara = self.unified_scanner.scan_all(
            decompile_dir, progress_cb=_unified_progress,
        )

        # Manifest + XML-based scanners (separate from smali)
        manifest_path = decompile_dir / "AndroidManifest.xml"
        if manifest_path.exists():
            _manifest_str = str(manifest_path)
            for _scanner, _name in (
                (self.manifest_scanner,   "manifest_scanner"),
                (self.exported_scanner,   "exported_scanner"),
                (self.intent_scanner,     "intent_scanner"),
                (self.permission_scanner, "permission_scanner"),
                (self.cleartext_scanner,  "cleartext_scanner"),
            ):
                try:
                    all_signals.extend(_scanner.scan(_manifest_str))
                except Exception as exc:
                    logger.warning(f"  {_name} skipped: {exc}")

        netsec_path = decompile_dir / "res" / "xml" / "network_security_config.xml"
        if netsec_path.exists():
            try:
                all_signals.extend(self.netsec_scanner.scan(str(netsec_path)))
            except Exception as exc:
                logger.warning(f"  netsec_scanner skipped: {exc}")

        logger.info(f"  {len(all_signals)} vulnerability signals")
        _progress(72, f"{len(all_signals)} signals + {len(smali_ara)} ARA — inferring findings")

        # ── [2b] JADX Java source scan (vuln only — ARA stays in smali) ────
        _progress(72, "Scanning JADX Java sources for vulnerability evidence")
        logger.info("[2b/8] JADX Java Source Scan (vulnerability evidence)")
        jadx_dir = adapter_ctx.jadx_dir
        if jadx_dir.exists() and (jadx_dir / "sources").exists():
            def _jadx_progress(pct, msg):
                _progress(72 + pct * 0.02, msg)

            jadx_signals = self.jadx_scanner.scan_all(
                jadx_dir, progress_cb=_jadx_progress,
            )
            all_signals.extend(jadx_signals)
            logger.info(f"  [jadx] {len(jadx_signals)} Java-source vulnerability signals")
        else:
            logger.info("  [jadx] No JADX sources — skipping Java scan")
        _progress(74, f"{len(all_signals)} total signals — inferring findings")

        # ── [3] Vuln findings ──────────────────────────────────────────────
        logger.info("[3/8] Vulnerability Findings")
        evidence_findings  = self.vuln_engine.scan_from_evidence(evidence)
        signal_findings    = self._signals_to_findings(all_signals)
        owasp_from_signals = {f.owasp_id for f in signal_findings}
        evidence_only      = [f for f in evidence_findings if f.owasp_id not in owasp_from_signals]
        vulnerabilities    = signal_findings + evidence_only
        logger.info(f"  {len(vulnerabilities)} vulnerability finding(s)")
        _progress(74, f"{len(vulnerabilities)} vulnerabilities — ARA native/ADB")

        # ── [4] ARA Scan — native + ADB only (smali already done above) ───
        logger.info("[4/8] ARA Scan (native + ADB — smali via unified scan)")
        ara_findings: List[ARAFinding] = list(smali_ara)
        logger.info(f"  [smali] {len(smali_ara)} ARA signals (from unified scan)")

        _progress(76, f"ARA scan — native layer ({len(smali_ara)} smali signals)")
        native_ara = self.native_ara_analyzer.analyze_dir(adapter_ctx.native_dir)
        ara_findings.extend(native_ara)
        logger.info(f"  [native] {len(native_ara)} ARA signals")

        # ── [4b] Call Graph / Fingerprint Analysis ─────────────────────────
        _progress(77, "Running Fingerprint & Call Graph analysis")
        logger.info("[4b/8] Fingerprint & Call Graph Analysis")
        fingerprint_matches = []
        try:
            # Try APK-level CG analysis first (Androguard)
            if base_apk.exists() and self.callgraph_analyzer._androguard_available:
                fingerprint_matches = self.callgraph_analyzer.analyze_apk(base_apk)
            else:
                # Fallback to smali-based fingerprint matching
                fingerprint_matches = self.callgraph_analyzer.analyze_smali(decompile_dir)

            # Convert CG fingerprint matches to ARAFinding for legacy pipeline
            cg_ara = self.callgraph_analyzer.matches_to_ara_findings(fingerprint_matches)
            ara_findings.extend(cg_ara)
            logger.info(f"  [CG] {len(fingerprint_matches)} fingerprints matched, "
                        f"{len(cg_ara)} ARA findings generated")
        except Exception as exc:
            logger.warning(f"  [CG] Fingerprint analysis failed: {exc}")

        # ── [4c] ADB Dynamic Checks (device-state — informational only) ──
        # NOTE: ADB checks probe the TESTER'S DEVICE (Magisk installed,
        # Frida running, emulator props, etc.) — NOT the APK's code.
        # These are stored as metadata/device_state but EXCLUDED from the
        # ARA protection profile to prevent false positives.
        package_name = metadata.get("package_name")
        if self.adb_adapter.is_available():
            _progress(78, "ADB device-state scan (informational)")
            logger.info("  [ADB] Device connected — device-state checks (NOT ARA)")
            adb_ara = self.adb_adapter.analyze(package_name=package_name)
            # Store as device_state metadata — do NOT add to ara_findings
            metadata["device_state_findings"] = [
                {
                    "category":  f.category,
                    "subtype":   f.subtype,
                    "technique": f.technique,
                    "confidence": f.confidence,
                    "matched_keyword": f.matched_keyword,
                }
                for f in adb_ara
            ]
            logger.info(f"  [ADB] {len(adb_ara)} device-state signals (excluded from ARA)")
            device_info = self.adb_adapter.get_device_info()
            if device_info:
                metadata["device"] = device_info
        else:
            logger.info("  [ADB] No device — dynamic analysis skipped")
            metadata["adb_available"] = False

        # ── [4d] Extended Dynamic Analysis (Monkey, logcat, ptrace, etc.) ──
        dynamic_report = None
        if self.dynamic_analyzer.is_available() and package_name:
            _progress(79, "Running extended dynamic analysis (Monkey + logcat + ptrace)")
            logger.info("[4d/8] Extended Dynamic Analysis")
            try:
                def _dyn_progress(pct, msg):
                    _progress(79 + pct * 0.05, f"[DYN] {msg}")

                dynamic_report = self.dynamic_analyzer.analyze(
                    package_name=package_name,
                    apk_path=str(base_apk) if base_apk.exists() else None,
                    install=True,        # auto-install if not on device
                    run_monkey=True,
                    progress_cb=_dyn_progress,
                )
                # Convert dynamic findings to ARAFinding for legacy pipeline
                dyn_ara = self.dynamic_analyzer.to_ara_findings(dynamic_report)
                ara_findings.extend(dyn_ara)
                logger.info(f"  [DYN] {len(dynamic_report.all_findings)} dynamic findings → "
                            f"{len(dyn_ara)} ARA findings")
            except Exception as exc:
                import traceback
                logger.warning(f"  [DYN] Extended dynamic analysis failed: {exc}")
                logger.debug(traceback.format_exc())
        else:
            logger.info(f"  [DYN] Extended dynamic analysis skipped "
                        f"(device={self.dynamic_analyzer.is_available()}, "
                        f"package={package_name!r})")

        logger.info(f"  Total ARA findings (pre-dedup): {len(ara_findings)}")

        # ── Cross-layer deduplication ──────────────────────────────────────
        # Multiple layers (smali, native, CG, dynamic) may detect the same
        # technique.  Keep only the highest-confidence finding per
        # (category, technique) pair to prevent evidence inflation.
        _dedup: dict = {}
        for f in ara_findings:
            key = (f.category, f.technique)
            existing = _dedup.get(key)
            if existing is None or f.confidence > existing.confidence:
                _dedup[key] = f
        ara_findings = list(_dedup.values())
        logger.info(f"  Total ARA findings (post-dedup): {len(ara_findings)}")

        # ── [5] ARA Profile ────────────────────────────────────────────────
        _progress(80, "Building ARA protection profile")
        logger.info("[5/8] ARA Profile Build")
        ara_summaries = build_ara_summary_from_findings(ara_findings)
        ara_profile   = self._build_ara_profile(ara_summaries)
        ara_dict      = ara_profile.as_dict()
        # Enrich ARA profile with evidence_count, confidence, difficulty from summaries
        for cat, summary in ara_summaries.items():
            if cat in ara_dict and isinstance(ara_dict[cat], dict):
                ara_dict[cat].setdefault("evidence_count", summary.get("evidence_count", 0))
                ara_dict[cat].setdefault("confidence", summary.get("confidence", 0.0))
                if not ara_dict[cat].get("subtypes"):
                    ara_dict[cat]["subtypes"] = summary.get("subtypes", [])
                if not ara_dict[cat].get("locations"):
                    ara_dict[cat]["locations"] = summary.get("locations", [])
        detected = [k for k, v in ara_dict.items() if isinstance(v, dict) and v.get("present")]
        logger.info(f"  ARA detected: {detected}")

        # ── [5b] Summary Analyzer (cross-layer fusion) ─────────────────────
        _progress(82, "Running cross-layer Summary Analyzer")
        logger.info("[5b/8] Summary Analyzer (static + CG + dynamic fusion)")
        try:
            summary_report = self.summary_analyzer.analyze(
                static_findings=ara_findings,
                fingerprint_matches=fingerprint_matches,
                dynamic_report=dynamic_report,
            )
            # Merge summary assessments back into ARA summaries
            ara_summaries = self.summary_analyzer.merge_into_ara_summary(
                summary_report, ara_summaries,
            )
            summary_dict = summary_report.as_dict()
            logger.info(f"  [SUMMARY] {summary_report.total_findings} total, "
                        f"coverage={summary_report.attack_surface.coverage_pct:.0f}%")
        except Exception as exc:
            logger.warning(f"  [SUMMARY] Summary analysis failed: {exc}")
            summary_dict = {}

        # ── [6] Correlation ────────────────────────────────────────────────
        _progress(84, "Correlating ARA posture with vulnerability findings")
        logger.info("[6/8] Correlation")
        normalized = self._normalize_for_correlation(ara_dict)
        correlated  = self.correlation_engine.correlate(vulnerabilities, normalized)

        # ── [7] Risk ───────────────────────────────────────────────────────
        _progress(88, "Calculating risk score")
        logger.info("[7/8] Risk Scoring")
        # Count how many ARA categories are actively present (not just detected
        # as absent).  This drives the ARA-absence exploitability bonus.
        _ara_present = sum(
            1 for v in ara_dict.values()
            if isinstance(v, dict) and v.get("present", False)
        )
        risk_score = self.risk_engine.calculate(correlated, ara_present_count=_ara_present)

        # ── [8] Report ─────────────────────────────────────────────────────
        _progress(92, "Generating security report")
        logger.info("[8/8] Report Generation")
        report = self.report_builder.build(
            metadata=metadata,
            ara=ara_dict,
            ara_findings=[f.as_dict() for f in ara_findings],
            vulnerabilities=vulnerabilities,
            correlated_findings=correlated,
            risk_score=risk_score,
        )
        # Inject summary data for the HTML report
        report["summary"] = summary_dict
        report["_pipeline_version"] = PIPELINE_VERSION

        # Inject raw vulnerability signals for the detailed findings section
        report["raw_signals"] = [s.as_dict() for s in all_signals]

        # ── Semantic Aggregation (noise reduction) ─────────────────────────
        # Converts raw signals into Distinct Vulnerabilities via the M-ILEA
        # 5-level hierarchy.  This NEVER deletes evidence — only changes
        # how signals are grouped and presented.
        from engines.vulnerability.aggregator import SemanticAggregator
        _aggregator = SemanticAggregator()
        aggregated = _aggregator.aggregate(
            report["raw_signals"],
            ara_posture=ara_dict,
        )
        report["aggregated_vulnerabilities"] = [av.as_dict() for av in aggregated]
        logger.info(
            f"  [AGG] {len(all_signals)} signals → {len(aggregated)} distinct vulnerabilities"
        )

        html = self.html_generator.generate(report)

        # Save to APK-level cache for instant future re-analysis
        self._save_apk_cache(workspace.parent, apk_sha256, report)

        _progress(100, "Analysis complete")
        return {"report": report, "html": html}

    # ==================================================================
    # APK ANALYSIS CACHE  (keyed by SHA-256 of APK content)
    # ==================================================================

    def _cache_path(self, workspaces_root: Path, sha256: str) -> Path:
        cache_dir = workspaces_root / ".apk_cache"
        cache_dir.mkdir(parents=True, exist_ok=True)
        return cache_dir / f"{sha256}.json"

    def _check_apk_cache(self, workspaces_root: Path, sha256: str) -> Optional[Dict]:
        import json as _json
        p = self._cache_path(workspaces_root, sha256)
        if p.exists():
            try:
                with open(p, "r", encoding="utf-8") as _f:
                    result = _json.load(_f)
                logger.info(f"  [cache] HIT for {sha256[:16]}… ({p.name})")
                return result
            except Exception as _exc:
                logger.warning(f"  [cache] read error, ignoring: {_exc}")
                try:
                    p.unlink()
                except Exception:
                    pass
        return None

    def _save_apk_cache(self, workspaces_root: Path, sha256: str, report: dict) -> None:
        import json as _json
        p = self._cache_path(workspaces_root, sha256)
        try:
            with open(p, "w", encoding="utf-8") as _f:
                _json.dump(report, _f, default=str)
            logger.info(f"  [cache] saved → {p.name}")
        except Exception as _exc:
            logger.warning(f"  [cache] write error: {_exc}")

    # ==================================================================
    # DECOMPILATION ARTIFACT CHECK
    # ==================================================================

    def _decompile_artifacts_exist(self, ctx: AdapterContext) -> bool:
        """Return True if a previous apktool run left valid artifacts."""
        d = ctx.decompile_dir
        if not (d / "AndroidManifest.xml").exists():
            return False
        smali_dirs = [x for x in d.iterdir() if x.is_dir() and x.name.startswith("smali")]
        if not smali_dirs:
            return False
        # Quick check: at least one .smali file must exist
        for sd in smali_dirs[:2]:
            if next(sd.rglob("*.smali"), None) is not None:
                return True
        return False

    # ==================================================================
    # METADATA ENRICHMENT
    # ==================================================================

    def _enrich_metadata(self, decompile_dir: Path, apk_path: Path, metadata: dict) -> None:
        """Parse AndroidManifest.xml, APK signing cert, and Play Store. File hashes are
        pre-computed by analyze() before this is called, so we skip them here."""
        import xml.etree.ElementTree as ET
        import zipfile, hashlib, subprocess

        # ── File hashes — only if NOT pre-computed by analyze() ────────────
        if not metadata.get("file_sha256"):
            try:
                stat = apk_path.stat()
                size_bytes = stat.st_size
                metadata.setdefault("file_name",       apk_path.name)
                metadata.setdefault("file_size_bytes", size_bytes)
                metadata.setdefault("file_size_mb",    f"{size_bytes / (1024 * 1024):.2f} MB")
                md5_h = hashlib.md5(); sha1_h = hashlib.sha1(); sha256_h = hashlib.sha256()
                with open(apk_path, "rb") as _f:
                    for _chunk in iter(lambda: _f.read(65536), b""):
                        md5_h.update(_chunk); sha1_h.update(_chunk); sha256_h.update(_chunk)
                metadata.setdefault("file_md5",    md5_h.hexdigest().upper())
                metadata.setdefault("file_sha1",   sha1_h.hexdigest().upper())
                metadata.setdefault("file_sha256", sha256_h.hexdigest().upper())
                logger.info(f"  [meta] file={apk_path.name} size={metadata['file_size_mb']}")
            except Exception as exc:
                logger.warning(f"  [meta] file hash failed: {exc}")

        # ── AndroidManifest.xml ─────────────────────────────────────────────
        manifest_path = decompile_dir / "AndroidManifest.xml"
        if manifest_path.exists():
            try:
                tree = ET.parse(manifest_path)
                root = tree.getroot()
                NS = "http://schemas.android.com/apk/res/android"

                pkg = root.get("package", "")
                if pkg:
                    metadata.setdefault("package_name", pkg)

                vc = root.get(f"{{{NS}}}versionCode", "")
                if vc:
                    try:
                        metadata.setdefault("version_code", int(vc))
                    except ValueError:
                        metadata.setdefault("version_code", vc)

                vn = root.get(f"{{{NS}}}versionName", "")
                if vn:
                    metadata.setdefault("version_name", vn)

                # <uses-sdk android:minSdkVersion=".." android:targetSdkVersion=".." />
                for elem in root.iter():
                    if elem.tag.endswith("uses-sdk"):
                        min_sdk = elem.get(f"{{{NS}}}minSdkVersion", "")
                        if min_sdk:
                            metadata.setdefault("min_sdk", min_sdk)
                        tgt_sdk = elem.get(f"{{{NS}}}targetSdkVersion", "")
                        if tgt_sdk:
                            metadata.setdefault("target_sdk", tgt_sdk)

                # <application android:label=".." /> — app name
                for elem in root.iter():
                    if elem.tag.endswith("application"):
                        lbl = elem.get(f"{{{NS}}}label", "")
                        if lbl and not lbl.startswith("@"):
                            metadata.setdefault("app_name", lbl)
                        break

                # Main activity — activity with MAIN + LAUNCHER intent-filter
                for activity in root.iter():
                    if not activity.tag.endswith("activity"):
                        continue
                    has_main = has_launcher = False
                    for child in activity:
                        if child.tag.endswith("intent-filter"):
                            for item in child:
                                item_name = item.get(f"{{{NS}}}name", "")
                                if item_name == "android.intent.action.MAIN":
                                    has_main = True
                                if item_name == "android.intent.category.LAUNCHER":
                                    has_launcher = True
                    if has_main and has_launcher:
                        act_name = activity.get(f"{{{NS}}}name", "")
                        if act_name:
                            metadata.setdefault("main_activity", act_name)
                        break

                logger.info(
                    f"  [meta] pkg={metadata.get('package_name')} "
                    f"v={metadata.get('version_name')} "
                    f"vc={metadata.get('version_code')} "
                    f"main={metadata.get('main_activity','?')}"
                )
            except Exception as exc:
                logger.warning(f"  [meta] manifest parse failed: {exc}")

        # ── Resolve app_name via aapt (handles @string/ refs) ───────────────
        if not metadata.get("app_name"):
            try:
                import re as _re
                for _aapt_bin in ("aapt", "aapt2"):
                    _aapt_res = subprocess.run(
                        [_aapt_bin, "dump", "badging", str(apk_path)],
                        capture_output=True, timeout=30,
                    )
                    if _aapt_res.returncode == 0:
                        _aapt_out = _aapt_res.stdout.decode("utf-8", errors="replace")
                        _lbl_m = _re.search(r"application-label:'([^']+)'", _aapt_out)
                        if _lbl_m:
                            metadata["app_name"] = _lbl_m.group(1)
                            logger.info(f"  [meta] app_name (aapt) = {metadata['app_name']}")
                        break
            except Exception as _exc:
                logger.debug(f"  [meta] aapt label extraction failed: {_exc}")

        # ── Signing Certificate ─────────────────────────────────────────────
        try:
            with zipfile.ZipFile(apk_path, "r") as zf:
                cert_files = [
                    n for n in zf.namelist()
                    if n.startswith("META-INF/") and n.upper().endswith((".RSA", ".DSA", ".EC"))
                ]
                if cert_files:
                    cert_bytes = zf.read(cert_files[0])
                    try:
                        result = subprocess.run(
                            ["openssl", "pkcs7", "-inform", "DER", "-noout",
                             "-print_certs", "-text"],
                            input=cert_bytes, capture_output=True, timeout=10,
                        )
                        out = result.stdout.decode(errors="replace")
                        for line in out.splitlines():
                            if "SHA256" in line.upper() and ":" in line:
                                fp = line.strip().replace("SHA256 Fingerprint=", "").strip()
                                metadata.setdefault("cert_sha256", fp)
                                break
                        for line in out.splitlines():
                            if "Subject:" in line:
                                metadata.setdefault("cert_subject", line.split("Subject:", 1)[-1].strip())
                            if "Issuer:" in line:
                                metadata.setdefault("cert_issuer", line.split("Issuer:", 1)[-1].strip())
                    except Exception:
                        raw_fp = hashlib.sha256(cert_bytes).hexdigest().upper()
                        fp_fmt = ":".join(raw_fp[i:i+2] for i in range(0, len(raw_fp), 2))
                        metadata.setdefault("cert_sha256", fp_fmt)
                    metadata.setdefault("cert_file", cert_files[0].split("/")[-1])
                    logger.info(f"  [meta] cert={metadata.get('cert_file')} fp={str(metadata.get('cert_sha256',''))[:20]}...")
        except Exception as exc:
            logger.warning(f"  [meta] cert extraction failed: {exc}")

        # ── Play Store ──────────────────────────────────────────────────────
        _pkg = metadata.get("package_name")
        if _pkg:
            try:
                from google_play_scraper import app as _gps_app
                _gps = _gps_app(_pkg, lang="en", country="us")
                metadata.setdefault("playstore", {
                    "title":             _gps.get("title", ""),
                    "score":             _gps.get("score"),
                    "ratings":           _gps.get("ratings"),
                    "installs":          _gps.get("installs", ""),
                    "price":             _gps.get("price", 0),
                    "free":              _gps.get("free", True),
                    "android_version":   _gps.get("androidVersion", ""),
                    "genre":             _gps.get("genre", ""),
                    "url":               _gps.get("url", ""),
                    "developer":         _gps.get("developer", ""),
                    "developer_id":      _gps.get("developerId", ""),
                    "developer_address": _gps.get("developerAddress", ""),
                    "developer_website": _gps.get("developerWebsite", ""),
                    "developer_email":   _gps.get("developerEmail", ""),
                    "released":          _gps.get("released", ""),
                    "privacy_policy":    _gps.get("privacyPolicy", ""),
                    "description":       (_gps.get("description") or "")[:600],
                })
                logger.info(
                    f"  [meta] Play Store: {_gps.get('title')} "
                    f"score={_gps.get('score')} installs={_gps.get('installs')}"
                )
            except Exception as exc:
                logger.info(f"  [meta] Play Store fetch skipped ({_pkg}): {exc}")
                metadata.setdefault("playstore", None)

    # ==================================================================
    # ARA PROFILE BUILDER
    # ==================================================================

    def _build_ara_profile(self, summaries: Dict[str, dict]):
        def _get(cat):
            return summaries.get(cat, {})

        ssl_s  = _get("SSL_PINNING")
        root_s = _get("ROOT_DETECTION")
        ai_s   = _get("ANTI_INSTRUMENTATION")
        at_s   = _get("ANTI_TAMPERING")
        em_s   = _get("EMULATOR_DETECTION")
        al_s   = _get("ALVD")

        emulator_arg = None
        if em_s.get("present"):
            emulator_arg = {
                "present":        True,
                "signal_score":   em_s.get("signal_score", 0),
                "decision_score": em_s.get("decision_score", 0),
                "signals":        em_s.get("signals_dict", {}),
                "posture":        em_s.get("posture", "DETECTED"),
                "style":          em_s.get("style", "STATIC"),
                "subtypes":       em_s.get("subtypes", []),
                "confidence":     em_s.get("confidence", 0.0),
                "evidence_count": em_s.get("evidence_count", 0),
            }

        at_arg = None
        if at_s.get("present"):
            at_arg = {
                "present":        True,
                "evidence_count": at_s.get("evidence_count", 0),
                "confidence":     at_s.get("confidence", 0.0),
                "subtypes":       at_s.get("subtypes", []),
                "posture":        at_s.get("posture", "DETECTED"),
            }

        alvd_arg = None
        if al_s.get("present"):
            alvd_arg = {
                "present":        True,
                "signal_score":   al_s.get("signal_score", 0),
                "signals":        al_s.get("signals_dict", {}),
                "posture":        al_s.get("posture", "DETECTED"),
                "subtypes":       al_s.get("subtypes", []),
                "confidence":     al_s.get("confidence", 0.0),
                "evidence_count": al_s.get("evidence_count", 0),
            }

        ssl_arg = {}
        if ssl_s.get("present"):
            ssl_arg = {
                "present":        True,
                "evidence_count": ssl_s.get("evidence_count", 0),
                "confidence":     ssl_s.get("confidence", 0.0),
                "subtypes":       ssl_s.get("subtypes", []),
            }

        root_arg = {}
        if root_s.get("present"):
            root_arg = {
                "present":        True,
                "evidence_count": root_s.get("evidence_count", 0),
                "confidence":     root_s.get("confidence", 0.0),
                "subtypes":       root_s.get("subtypes", []),
            }

        ai_arg = {}
        if ai_s.get("present"):
            ai_arg = {
                "present":    True,
                "signals":    ai_s.get("signals", 0),
                "subtypes":   ai_s.get("subtypes", []),
                "confidence": ai_s.get("confidence", 0.0),
                "evidence":   ai_s.get("evidence_count", 0),
            }

        return self.profiler.build(
            ssl_summary=ssl_arg,
            root_summary=root_arg,
            anti_instr_posture=ai_arg,
            anti_tampering=at_arg,
            emulator=emulator_arg,
            alvd_summary=alvd_arg,
        )

    # ==================================================================
    # NORMALIZATION
    # ==================================================================

    def _normalize_for_correlation(self, ara: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        normalized = {}
        for name, data in ara.items():
            if not isinstance(data, dict):
                continue
            present    = data.get("present", False)
            difficulty = data.get("difficulty", "NONE")
            if not present:
                difficulty = "NONE"
            elif difficulty not in ("LOW", "MEDIUM", "HIGH"):
                difficulty = "LOW"
            normalized[name] = {"present": present, "difficulty": difficulty}
        return normalized

    # ==================================================================
    # SIGNAL -> FINDING CONVERTER
    # ==================================================================

    _OWASP_META: Dict[str, tuple] = {
        "M1": (
            "IMPROPER_CREDENTIAL_USAGE",
            "M1 — Improper Credential Usage",
            "HIGH",
            "Hardcoded or improperly managed credentials found in application code. "
            "Attackers can extract secrets by decompiling the APK.",
            "Remove hardcoded secrets. Use Android Keystore. Rotate any exposed credentials immediately.",
        ),
        # M1 true-credential subtypes used for smart description selection
        "_M1_TRUE_CREDENTIAL_SUBTYPES": (
            "hardcoded_password", "hardcoded_api_key", "hardcoded_secret",
            "hardcoded_private_key", "hardcoded_aws_key", "hardcoded_google_key",
        ),
        "M2": (
            "INADEQUATE_SUPPLY_CHAIN_SECURITY",
            "M2 — Inadequate Supply Chain Security",
            "HIGH",
            "Third-party libraries or SDKs included in the application may contain "
            "known vulnerabilities, expanding the attack surface.",
            "Audit all dependencies. Pin library versions and verify checksums.",
        ),
        "M3": (
            "INSECURE_AUTHENTICATION",
            "M3 — Insecure Authentication / Authorization",
            "HIGH",
            "Authentication tokens appear to be stored or validated insecurely, "
            "enabling session hijacking or privilege escalation.",
            "Use strong token algorithms (JWT RS256/EdDSA). Enforce server-side validation.",
        ),
        "M4": (
            "INSUFFICIENT_INPUT_VALIDATION",
            "M4 — Insufficient Input / Output Validation",
            "MEDIUM",
            "User-supplied data processed without adequate validation may enable "
            "injection or logic-bypass attacks.",
            "Apply strict allowlist input validation. Sanitize all external data.",
        ),
        "M5": (
            "INSECURE_COMMUNICATION",
            "M5 — Insecure Communication",
            "HIGH",
            "Application permits cleartext traffic or trusts untrusted CAs, "
            "exposing users to MITM interception.",
            "Enforce TLS 1.2+. Set cleartextTrafficPermitted=false. Implement certificate pinning.",
        ),
        "M6": (
            "INADEQUATE_PRIVACY_CONTROLS",
            "M6 — Inadequate Privacy Controls",
            "HIGH",
            "PII (device ID, location, advertising ID) collected or transmitted "
            "without adequate consent or protection.",
            "Minimise PII collection. Encrypt PII in transit and at rest.",
        ),
        "M7": (
            "INSUFFICIENT_BINARY_PROTECTIONS",
            "M7 — Insufficient Binary Protections",
            "MEDIUM",
            "Aggregated indicators suggest inadequate binary hardening posture, "
            "potentially facilitating reverse engineering.",
            "Enable ProGuard/R8. Integrate RASP. Enforce code signing validation at runtime.",
        ),
        "M8": (
            "SECURITY_MISCONFIGURATION",
            "M8 — Security Misconfiguration",
            "MEDIUM",
            "Insecure manifest settings (debuggable=true, allowBackup=true, exported "
            "components) expand the attack surface.",
            "Disable debuggable and allowBackup in production. Restrict exported components.",
        ),
        "M9": (
            "INSECURE_DATA_STORAGE",
            "M9 — Insecure Data Storage",
            "HIGH",
            "Sensitive data stored in plaintext SharedPreferences, world-readable files, "
            "or external storage without encryption.",
            "Use EncryptedSharedPreferences or Android Keystore for all sensitive data.",
        ),
        "M10": (
            "INSUFFICIENT_CRYPTOGRAPHY",
            "M10 — Insufficient Cryptography",
            "HIGH",
            "Deprecated or broken algorithms (MD5, SHA1, AES/ECB) provide inadequate security.",
            "Replace with AES/GCM, SHA-256, and TLS 1.3. Use authenticated encryption.",
        ),
    }

    def _signals_to_findings(
        self, signals: List[VulnerabilitySignal]
    ) -> List[VulnerabilityFinding]:
        from collections import defaultdict

        by_owasp: Dict[str, List[VulnerabilitySignal]] = defaultdict(list)
        for s in signals:
            by_owasp[s.owasp_id].append(s)

        findings = []
        for owasp_id, group in by_owasp.items():
            meta = self._OWASP_META.get(owasp_id)
            if not meta:
                continue
            cat, title, severity, description, remediation = meta

            # ── Hybrid evidence: prefer Java (JADX) for readability ───
            jadx_sigs  = [s for s in group if s.source == "jadx"]
            smali_sigs = [s for s in group if s.source != "jadx"]
            primary    = jadx_sigs if jadx_sigs else smali_sigs
            secondary  = smali_sigs if jadx_sigs else []

            # Affected files: prefer Java paths
            affected = list({s.file for s in primary})
            if not affected:
                affected = list({s.file for s in group})

            # Build evidence: Java first, then smali as fallback
            evidence: List[str] = []
            for s in primary[:15]:
                if s.evidence:
                    evidence.extend(s.evidence[:2])
                elif s.file and s.line:
                    evidence.append(f"{s.file}:{s.line} — {s.subtype}")
                elif s.code:
                    evidence.append(s.code)

            # Supplement with secondary source if needed
            if len(evidence) < 5 and secondary:
                for s in secondary[:5]:
                    if s.evidence:
                        evidence.extend(s.evidence[:2])
                    elif s.file and s.line:
                        evidence.append(f"{s.file}:{s.line} — {s.subtype}")

            # Deduplicate while preserving order
            seen_ev = set()
            unique_evidence = []
            for e in evidence:
                if e not in seen_ev:
                    seen_ev.add(e)
                    unique_evidence.append(e)
            evidence = unique_evidence[:15]

            confidence = min(0.97, max(s.confidence for s in group))
            total_signals = len(group)

            # ── M1 smart description: soft wording when no real credentials ──
            final_description = description
            if owasp_id == "M1":
                true_cred_subs = self._OWASP_META.get("_M1_TRUE_CREDENTIAL_SUBTYPES", ())
                group_subtypes = {s.subtype for s in group}
                has_true_creds = bool(group_subtypes & set(true_cred_subs))
                if not has_true_creds:
                    final_description = (
                        "No hardcoded credentials were found. "
                        "Findings indicate potential token-handling or session-identifier patterns."
                    )

            findings.append(VulnerabilityFinding(
                owasp_id=owasp_id,
                title=title,
                category=cat,
                subtype=group[0].subtype,
                severity=severity,
                confidence=confidence,
                description=f"{final_description} (supported by {total_signals} evidences)",
                recommendation=remediation,
                remediation=remediation,
                evidence=evidence,
                affected_files=affected,
                signal_count=total_signals,
            ))
        return findings

    # ==================================================================
    # HELPERS
    # ==================================================================

    def _run_adapters(self, ctx: AdapterContext) -> None:
        for adapter in (self.apktool, self.jadx, self.native):
            result = adapter.run(ctx)
            if not result.success:
                raise RuntimeError(f"{adapter.__class__.__name__} failed: {result.error}")

    def _resolve_base_apk(self, workspace: Path) -> Path:
        uploads = workspace / "uploads"
        if not uploads.exists():
            raise RuntimeError("Uploads directory not found")
        apks = list(uploads.glob("*.apk"))
        if not apks:
            raise RuntimeError("No APK found in uploads directory")

        # 1. Prefer explicitly named base.apk / universal.apk
        for preferred in ("base.apk", "universal.apk"):
            for apk in apks:
                if apk.name.lower() == preferred:
                    logger.info(f"  [base-apk] selected by name: {apk.name}")
                    return apk

        # 2. Any APK whose name does NOT contain "split"
        non_split = [a for a in apks if "split" not in a.name.lower()]
        if non_split:
            chosen = max(non_split, key=lambda p: p.stat().st_size)
            logger.info(f"  [base-apk] selected non-split (largest): {chosen.name}")
            return chosen

        # 3. Last resort: pick the largest APK (most likely to contain code)
        chosen = max(apks, key=lambda p: p.stat().st_size)
        logger.info(f"  [base-apk] fallback to largest: {chosen.name}")
        return chosen


# ---------------------------------------------------------------------------
# Backward-compatible lightweight orchestrator for test_vulnerability_orchestrator
# ---------------------------------------------------------------------------

from engines.vulnerability.vulnerability_inferer import VulnerabilityInfererV1 as _VIV1
from engines.vulnerability.models import VulnerabilitySignal as _VS


class VulnerabilityOrchestrator:
    """Simple signal → finding router used by legacy unit tests."""

    # Normalise subtypes so tests that send one variant get the expected variant back.
    _SUBTYPE_MAP = {
        "intent_extra_unvalidated": "intent_input_unvalidated",
        "intent_input_unvalidated": "intent_input_unvalidated",
    }

    def run(self, signals: list) -> list:
        # remap subtypes
        normalised = []
        for s in signals:
            mapped = self._SUBTYPE_MAP.get(s.subtype, s.subtype)
            if mapped != s.subtype:
                s = _VS(
                    owasp_id=s.owasp_id,
                    category=s.category,
                    subtype=mapped,
                    source=s.source,
                    file=s.file,
                    line=s.line,
                    confidence=s.confidence,
                    evidence=list(s.evidence) if hasattr(s, "evidence") and s.evidence else [],
                )
            normalised.append(s)

        inferer = _VIV1()
        findings = inferer.infer(normalised)
        return findings

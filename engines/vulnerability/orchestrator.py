# engines/vulnerability/orchestrator.py
"""
M-ILEA ORCHESTRATOR - FINAL PIPELINE
Stages: Vuln → Protection → Correlation → Risk → Report → HTML
"""

from typing import Dict, Any, Optional
from .engine import VulnerabilityEngine
from .correlation.engine import CorrelationEngine
from .risk.engine import RiskScoringEngine
from engines.protection.pipeline import ProtectionPipeline
from engines.report.builder import UnifiedReportBuilder
from engines.report.html_generator import UnifiedHTMLReportGenerator


class M_ILEAOrchestrator:
    """
    Master orchestrator for full M-ILEA pipeline.
    Enforces strict separation of concerns.
    """

    def __init__(self):
        self.vuln_engine = VulnerabilityEngine()
        self.protection_pipeline = ProtectionPipeline()
        self.correlation_engine = CorrelationEngine()
        self.risk_engine = RiskScoringEngine()
        self.report_builder = UnifiedReportBuilder()
        self.html_generator = UnifiedHTMLReportGenerator()

    def analyze(
        self,
        workspace_path: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Full 6-stage pipeline:
        1. VulnerabilityEngine - scan for vulnerabilities
        2. ProtectionPipeline - detect protections (ARA)
        3. CorrelationEngine - adjust severity based on protections
        4. RiskScoringEngine - aggregate risk
        5. UnifiedReportBuilder - assemble report
        6. UnifiedHTMLReportGenerator - generate HTML

        Returns: {report: dict, html: str}
        """
        if metadata is None:
            metadata = {"workspace": workspace_path, "version": "1.0"}

        print("[1/6] Running Vulnerability Engine...")
        vulnerabilities = self.vuln_engine.scan(workspace_path)
        print(f"  → Found {len(vulnerabilities)} vulnerabilities")

        print("[2/6] Running Protection Pipeline (ARA)...")
        protection_result = self.protection_pipeline.run(workspace_path)
        protection_profile = protection_result.get("profile") if isinstance(protection_result, dict) else protection_result
        ara_dict = protection_profile.as_dict() if hasattr(protection_profile, 'as_dict') else protection_profile
        print(f"  → Detected protections")

        print("[3/6] Running Correlation Engine...")
        correlated_findings = self.correlation_engine.correlate(
            vulnerabilities,
            protection_profile
        )
        print(f"  → Correlated {len(correlated_findings)} findings")

        print("[4/6] Running Risk Scoring Engine...")
        risk_score = self.risk_engine.calculate(correlated_findings)
        print(f"  → Risk Level: {risk_score['level']} (score: {risk_score['numeric']})")

        print("[5/6] Building Unified Report...")
        report = self.report_builder.build(
            metadata=metadata,
            ara=ara_dict,
            vulnerabilities=vulnerabilities,
            correlated_findings=correlated_findings,
            risk_score=risk_score,
        )
        print("  → Report assembled")

        print("[6/6] Generating HTML Report...")
        html = self.html_generator.generate(report)
        print("  → HTML generated")

        return {
            "report": report,
            "html": html,
        }

    def analyze_to_file(
        self,
        workspace_path: str,
        output_file: str = "report.html",
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Full pipeline with HTML file output
        """
        result = self.analyze(workspace_path, metadata)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(result["html"])
        print(f"\n✓ Report saved to {output_file}")
        return output_file

# engines/vulnerability/risk/engine.py

from typing import List, Dict, Any


class RiskScoringEngine:
    """
    STAGE 4: Confidence-Aware Risk Scoring Engine

    Input: List[CorrelatedFinding]
    Output:
      {
        numeric: float,
        level: str,
        explanation: list[str],
        breakdown: list[dict]
      }

    Responsibility:
      - Aggregate effective risk
      - Weight by confidence & mitigation
      - Explain WHY the risk is what it is
    """

    # ------------------------------------------------------------------
    # CONFIGURATION
    # ------------------------------------------------------------------

    SEVERITY_WEIGHT = {
        "CRITICAL": 4.0,
        "HIGH": 3.0,
        "MEDIUM": 2.0,
        "LOW": 1.0,
    }

    MITIGATION_MULTIPLIER = {
        "MITIGATED": 0.4,
        "PARTIALLY_MITIGATED": 0.7,
        "NOT_MITIGATED": 1.0,
    }

    DIFFICULTY_MULTIPLIER = {
        "LOW": 1.0,
        "MEDIUM": 0.9,
        "HIGH": 0.8,
        "UNKNOWN": 1.0,
    }

    # Compounding penalty: each additional independent finding broadens the
    # attack surface.  1.5 percentage points per finding beyond the first,
    # capped at 20 pp so a single high-sev finding still dominates.
    BREADTH_BONUS_PER_FINDING: float = 1.5
    BREADTH_BONUS_MAX: float = 20.0

    # ARA-absence bonus: an app with zero runtime self-protection layers
    # has maximum exploitability — attackers can freely instrument, debug,
    # and tamper without hindrance.  Each absent ARA category contributes
    # proportionally to this bonus, capped at MAX_ARA_ABSENCE_BONUS.
    # Formula: bonus = (absent_count / total_categories) × MAX
    #   All 6 absent → +15 pp (e.g. DIVA: 55.5 + 15.0 = 70.5 = CRITICAL)
    #   3/6 absent   → +7.5 pp
    #   0/6 absent   → +0 pp  (all ARA present — no bonus)
    ARA_ABSENCE_BONUS_MAX: float = 15.0
    ARA_TOTAL_CATEGORIES: int = 6  # SSL_PINNING, ROOT_DETECTION, ANTI_INSTR,
                                   # ANTI_TAMPERING, EMULATOR_DETECTION, ALVD

    # ------------------------------------------------------------------

    def calculate(
        self,
        correlated_findings: List,
        ara_present_count: int = 0,
        total_ara_categories: int = 6,
    ) -> Dict[str, Any]:
        """
        Calculate aggregate risk from correlated findings.

        Formula
        -------
        For each finding:
            score_i  = severity_weight × confidence × mit_mult × diff_mult
            max_i    = SEVERITY_WEIGHT["CRITICAL"] × 1.0 × 1.0 × 1.0   (= 4.0)

        base_normalized = (Σ score_i / Σ max_i) × 100

        Breadth bonus:
            bonus = min(BREADTH_BONUS_MAX, (N-1) × BREADTH_BONUS_PER_FINDING)

        normalized = min(100, base_normalized + bonus)

        Rationale: real-world scanner confidence tops out at 0.90, so a
        single HIGH finding can never exceed 75 % of its slot.  N independent
        vulnerabilities compound the attack surface; the breadth bonus
        reflects that compound exposure.

        Level thresholds (post-bonus including ARA-absence):
          < 22  → LOW
          22–45 → MEDIUM
          46–69 → HIGH
          70+   → CRITICAL
        """

        if not correlated_findings:
            return {
                "numeric": 0.0,
                "level": "LOW",
                "explanation": ["No vulnerabilities found"],
                "breakdown": [],
            }

        total_score: float = 0.0
        max_possible: float = 0.0
        explanations: list[str] = []
        breakdown: list[dict] = []

        mitigated_count = 0

        for finding in correlated_findings:
            # ----------------------------------------------------------
            # Severity
            # ----------------------------------------------------------
            severity = finding.effective_risk
            severity_weight = self.SEVERITY_WEIGHT.get(severity, 1.0)

            # ----------------------------------------------------------
            # Confidence (from original finding)
            # ----------------------------------------------------------
            confidence = finding.original_finding.get("confidence", 1.0)

            # ----------------------------------------------------------
            # Mitigation
            # ----------------------------------------------------------
            mitigation = finding.mitigation_status
            mitigation_multiplier = self.MITIGATION_MULTIPLIER.get(
                mitigation, 1.0
            )

            if mitigation == "MITIGATED":
                mitigated_count += 1

            # ----------------------------------------------------------
            # Difficulty (best-effort inference)
            # ----------------------------------------------------------
            difficulty = self._infer_difficulty(finding)
            difficulty_multiplier = self.DIFFICULTY_MULTIPLIER.get(
                difficulty, 1.0
            )

            # ----------------------------------------------------------
            # Final per-finding score
            # ----------------------------------------------------------
            score = (
                severity_weight
                * confidence
                * mitigation_multiplier
                * difficulty_multiplier
            )

            total_score += score
            # Max possible: CRITICAL severity, full confidence, unmitigated
            max_possible += self.SEVERITY_WEIGHT["CRITICAL"] * 1.0 * 1.0 * 1.0

            # ----------------------------------------------------------
            # Explanation
            # ----------------------------------------------------------
            explanations.append(
                f"{finding.owasp_id} {finding.title}: "
                f"{severity} × confidence({confidence}) × "
                f"{mitigation} × difficulty({difficulty}) = {score:.2f}"
            )

            breakdown.append(
                {
                    "owasp_id": finding.owasp_id,
                    "severity": severity,
                    "confidence": confidence,
                    "mitigation_status": mitigation,
                    "difficulty": difficulty,
                    "score": round(score, 3),
                }
            )

        # --------------------------------------------------------------
        # Normalize to 0–100 scale
        # --------------------------------------------------------------
        if max_possible > 0:
            base_normalized = (total_score / max_possible) * 100
        else:
            base_normalized = 0.0

        # Breadth bonus: N independent vulnerabilities compound attack surface
        n = len(correlated_findings)
        breadth_bonus = min(
            self.BREADTH_BONUS_MAX,
            max(0, n - 1) * self.BREADTH_BONUS_PER_FINDING,
        )

        # ARA-absence bonus: absent runtime protection → higher exploitability
        _total_cat = max(1, total_ara_categories)
        absent_count = max(0, _total_cat - max(0, ara_present_count))
        ara_absence_bonus = round(
            (absent_count / _total_cat) * self.ARA_ABSENCE_BONUS_MAX, 1
        )

        normalized = min(100.0, max(0.0, base_normalized + breadth_bonus + ara_absence_bonus))

        risk_level = self._level(normalized)

        explanations.insert(
            0,
            f"{mitigated_count}/{len(correlated_findings)} findings fully mitigated"
        )
        if ara_absence_bonus > 0:
            explanations.insert(
                0,
                f"ARA-absence bonus: {absent_count}/{_total_cat} categories unprotected "
                f"→ +{ara_absence_bonus:.1f}pp (exploitability multiplier)"
            )
        explanations.insert(
            0,
            f"Breadth bonus: {n} finding(s) × {self.BREADTH_BONUS_PER_FINDING}/finding "
            f"= +{breadth_bonus:.1f}pp  (base {base_normalized:.1f} → {normalized:.1f})"
        )
        explanations.insert(
            0,
            f"Overall application risk assessed as {risk_level} "
            f"({normalized:.0f}/100, raw {total_score:.1f}/{max_possible:.1f} "
            f"+ breadth {breadth_bonus:.1f} + ARA-absence {ara_absence_bonus:.1f})"
        )

        return {
            "numeric": round(normalized, 1),
            "level": risk_level,
            "explanation": explanations,
            "breakdown": breakdown,
        }

    # ------------------------------------------------------------------
    # HELPERS
    # ------------------------------------------------------------------

    def _infer_difficulty(self, finding) -> str:
        """
        Infer bypass difficulty from the correlation reasoning stored in the
        CorrelatedFinding. Reads the `reasoning` list for tier keywords
        (strong / medium / weak) set by the OPSI-A correlation engine.
        Falls back to mitigation_status when no tier keyword is present.
        """
        # Primary: read tier keywords from OPSI-A reasoning strings
        reasoning = getattr(finding, "reasoning", []) or []
        for entry in reasoning:
            entry_l = entry.lower()
            if "strong" in entry_l:
                return "HIGH"
            if "medium" in entry_l or "moderate" in entry_l:
                return "MEDIUM"
            if "weak" in entry_l:
                return "LOW"

        # Fallback: derive from mitigation_status
        status = getattr(finding, "mitigation_status", "NOT_MITIGATED")
        if status == "MITIGATED":
            return "HIGH"
        elif status == "PARTIALLY_MITIGATED":
            return "MEDIUM"
        return "LOW"

    def _level(self, score: float) -> str:
        """
        Convert normalized 0-100 score to semantic risk level.

        Calibrated for weight scale CRITICAL=4.0 / HIGH=3.0 / MEDIUM=2.0 / LOW=1.0.
        Theoretical maxima per severity:
          ALL CRITICAL, conf=1.0, NOT_MIT  → 100%
          ALL HIGH,     conf=1.0, NOT_MIT  →  75%
          ALL MEDIUM,   conf=1.0, NOT_MIT  →  50%
          ALL LOW,      conf=1.0, NOT_MIT  →  25%

        Thresholds:
          0–21  → LOW
          22–45 → MEDIUM
          46–69 → HIGH
          70+   → CRITICAL
        """
        if score >= 70.0:
            return "CRITICAL"
        elif score >= 46.0:
            return "HIGH"
        elif score >= 22.0:
            return "MEDIUM"
        else:
            return "LOW"


# Backwards-compatibility: older code imports `RiskEngine`
class RiskEngine(RiskScoringEngine):
    """Compatibility wrapper for legacy import name `RiskEngine`."""
    pass

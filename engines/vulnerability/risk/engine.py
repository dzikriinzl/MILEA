# engines/vulnerability/risk/engine.py

from typing import List, Dict, Any


class RiskScoringEngine:
    """
    STAGE 4: Confidence-Aware Risk Scoring Engine

    Input: List[CorrelatedFinding]
    Output:
      {
        numeric: float,
        level: str,
        explanation: list[str],
        breakdown: list[dict]
      }

    Responsibility:
      - Aggregate effective risk
      - Weight by confidence & mitigation
      - Explain WHY the risk is what it is
    """

    # ------------------------------------------------------------------
    # CONFIGURATION
    # ------------------------------------------------------------------

    SEVERITY_WEIGHT = {
        "CRITICAL": 4.0,
        "HIGH": 3.0,
        "MEDIUM": 2.0,
        "LOW": 1.0,
    }

    MITIGATION_MULTIPLIER = {
        "MITIGATED": 0.4,
        "PARTIALLY_MITIGATED": 0.7,
        "NOT_MITIGATED": 1.0,
    }

    DIFFICULTY_MULTIPLIER = {
        "LOW": 1.0,
        "MEDIUM": 0.9,
        "HIGH": 0.8,
        "UNKNOWN": 1.0,
    }

    # ------------------------------------------------------------------

    def calculate(self, correlated_findings: List) -> Dict[str, Any]:
        """
        Calculate aggregate risk from correlated findings.

        Returns a normalized 0–100 risk score:
          - Raw sum of per-finding weighted scores
          - Normalized against theoretical maximum for the finding set
          - Level thresholds: <25 LOW, <50 MEDIUM, <75 HIGH, ≥75 CRITICAL
        """

        if not correlated_findings:
            return {
                "numeric": 0.0,
                "level": "LOW",
                "explanation": ["No vulnerabilities found"],
                "breakdown": [],
            }

        total_score: float = 0.0
        max_possible: float = 0.0
        explanations: list[str] = []
        breakdown: list[dict] = []

        mitigated_count = 0

        for finding in correlated_findings:
            # ----------------------------------------------------------
            # Severity
            # ----------------------------------------------------------
            severity = finding.effective_risk
            severity_weight = self.SEVERITY_WEIGHT.get(severity, 1.0)

            # ----------------------------------------------------------
            # Confidence (from original finding)
            # ----------------------------------------------------------
            confidence = finding.original_finding.get("confidence", 1.0)

            # ----------------------------------------------------------
            # Mitigation
            # ----------------------------------------------------------
            mitigation = finding.mitigation_status
            mitigation_multiplier = self.MITIGATION_MULTIPLIER.get(
                mitigation, 1.0
            )

            if mitigation == "MITIGATED":
                mitigated_count += 1

            # ----------------------------------------------------------
            # Difficulty (best-effort inference)
            # ----------------------------------------------------------
            difficulty = self._infer_difficulty(finding)
            difficulty_multiplier = self.DIFFICULTY_MULTIPLIER.get(
                difficulty, 1.0
            )

            # ----------------------------------------------------------
            # Final per-finding score
            # ----------------------------------------------------------
            score = (
                severity_weight
                * confidence
                * mitigation_multiplier
                * difficulty_multiplier
            )

            total_score += score
            # Max possible: CRITICAL severity, full confidence, unmitigated
            max_possible += self.SEVERITY_WEIGHT["CRITICAL"] * 1.0 * 1.0 * 1.0

            # ----------------------------------------------------------
            # Explanation
            # ----------------------------------------------------------
            explanations.append(
                f"{finding.owasp_id} {finding.title}: "
                f"{severity} × confidence({confidence}) × "
                f"{mitigation} × difficulty({difficulty}) = {score:.2f}"
            )

            breakdown.append(
                {
                    "owasp_id": finding.owasp_id,
                    "severity": severity,
                    "confidence": confidence,
                    "mitigation_status": mitigation,
                    "difficulty": difficulty,
                    "score": round(score, 3),
                }
            )

        # --------------------------------------------------------------
        # Normalize to 0–100 scale
        # --------------------------------------------------------------
        if max_possible > 0:
            normalized = (total_score / max_possible) * 100
        else:
            normalized = 0.0
        normalized = min(100.0, max(0.0, normalized))

        risk_level = self._level(normalized)

        explanations.insert(
            0,
            f"{mitigated_count}/{len(correlated_findings)} findings fully mitigated"
        )
        explanations.insert(
            0,
            f"Overall application risk assessed as {risk_level} "
            f"({normalized:.0f}/100, raw sum {total_score:.1f}/{max_possible:.1f})"
        )

        return {
            "numeric": round(normalized, 1),
            "level": risk_level,
            "explanation": explanations,
            "breakdown": breakdown,
        }

    # ------------------------------------------------------------------
    # HELPERS
    # ------------------------------------------------------------------

    def _infer_difficulty(self, finding) -> str:
        """
        Infer bypass difficulty from correlation trace if available.
        Safe heuristic, never fatal.
        """
        trace = getattr(finding, "trace", None)
        if not trace:
            return "UNKNOWN"

        for log in trace.decision_log:
            log_l = log.lower()
            if "bypass" in log_l:
                if "high" in log_l:
                    return "HIGH"
                if "medium" in log_l:
                    return "MEDIUM"
                if "low" in log_l:
                    return "LOW"

        return "UNKNOWN"

    def _level(self, score: float) -> str:
        """
        Convert normalized 0-100 score to semantic risk level.

        Thresholds:
          0–24  → LOW
          25–49 → MEDIUM
          50–74 → HIGH
          75+   → CRITICAL
        """
        if score >= 75:
            return "CRITICAL"
        elif score >= 50:
            return "HIGH"
        elif score >= 25:
            return "MEDIUM"
        else:
            return "LOW"


# Backwards-compatibility: older code imports `RiskEngine`
class RiskEngine(RiskScoringEngine):
    """Compatibility wrapper for legacy import name `RiskEngine`."""
    pass
